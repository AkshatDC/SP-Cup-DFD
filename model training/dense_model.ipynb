{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_-tygsBa-DO",
        "outputId": "01f37137-d5aa-4484-c8f6-9dcfef5ecab5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No GPU detected, running on CPU.\n",
            "Loading training features...\n",
            "Augmenting training data...\n",
            "Shape of X_train: (202886, 1280)\n",
            "Shape of X_val: (25361, 1280)\n",
            "Shape of y_train: (202886,)\n",
            "Shape of y_val: (25361,)\n",
            "Starting training...\n",
            "Epoch 1/20\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 13ms/step - accuracy: 0.8700 - loss: 4.0647 - val_accuracy: 0.9998 - val_loss: 1.3094 - learning_rate: 1.0000e-04\n",
            "Epoch 2/20\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 13ms/step - accuracy: 0.9993 - loss: 0.8840 - val_accuracy: 0.9999 - val_loss: 0.1352 - learning_rate: 1.0000e-04\n",
            "Epoch 3/20\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 12ms/step - accuracy: 0.9995 - loss: 0.1070 - val_accuracy: 1.0000 - val_loss: 0.0487 - learning_rate: 1.0000e-04\n",
            "Epoch 4/20\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 13ms/step - accuracy: 0.9995 - loss: 0.0460 - val_accuracy: 0.9999 - val_loss: 0.0335 - learning_rate: 1.0000e-04\n",
            "Epoch 5/20\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 14ms/step - accuracy: 0.9991 - loss: 0.0497 - val_accuracy: 0.9999 - val_loss: 0.0314 - learning_rate: 1.0000e-04\n",
            "Epoch 6/20\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 12ms/step - accuracy: 0.9996 - loss: 0.0323 - val_accuracy: 0.9999 - val_loss: 0.0280 - learning_rate: 1.0000e-04\n",
            "Epoch 7/20\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 13ms/step - accuracy: 0.9994 - loss: 0.0311 - val_accuracy: 1.0000 - val_loss: 0.0246 - learning_rate: 1.0000e-04\n",
            "Epoch 8/20\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 13ms/step - accuracy: 0.9997 - loss: 0.0238 - val_accuracy: 1.0000 - val_loss: 0.0240 - learning_rate: 1.0000e-04\n",
            "Epoch 9/20\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 12ms/step - accuracy: 0.9995 - loss: 0.0275 - val_accuracy: 1.0000 - val_loss: 0.0259 - learning_rate: 1.0000e-04\n",
            "Epoch 10/20\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 12ms/step - accuracy: 0.9995 - loss: 0.0295 - val_accuracy: 0.9998 - val_loss: 0.0278 - learning_rate: 1.0000e-04\n",
            "Epoch 11/20\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9992 - loss: 0.0347\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 13ms/step - accuracy: 0.9992 - loss: 0.0347 - val_accuracy: 0.9999 - val_loss: 0.0244 - learning_rate: 1.0000e-04\n",
            "Epoch 12/20\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 12ms/step - accuracy: 0.9998 - loss: 0.0229 - val_accuracy: 1.0000 - val_loss: 0.0134 - learning_rate: 5.0000e-05\n",
            "Epoch 13/20\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 13ms/step - accuracy: 0.9997 - loss: 0.0144 - val_accuracy: 1.0000 - val_loss: 0.0112 - learning_rate: 5.0000e-05\n",
            "Epoch 14/20\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 13ms/step - accuracy: 0.9997 - loss: 0.0136 - val_accuracy: 1.0000 - val_loss: 0.0115 - learning_rate: 5.0000e-05\n",
            "Epoch 15/20\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 12ms/step - accuracy: 0.9997 - loss: 0.0146 - val_accuracy: 1.0000 - val_loss: 0.0107 - learning_rate: 5.0000e-05\n",
            "Epoch 16/20\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 12ms/step - accuracy: 0.9999 - loss: 0.0107 - val_accuracy: 1.0000 - val_loss: 0.0087 - learning_rate: 5.0000e-05\n",
            "Epoch 17/20\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 13ms/step - accuracy: 0.9997 - loss: 0.0127 - val_accuracy: 1.0000 - val_loss: 0.0116 - learning_rate: 5.0000e-05\n",
            "Epoch 18/20\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 12ms/step - accuracy: 0.9998 - loss: 0.0119 - val_accuracy: 1.0000 - val_loss: 0.0083 - learning_rate: 5.0000e-05\n",
            "Epoch 19/20\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 13ms/step - accuracy: 0.9997 - loss: 0.0108 - val_accuracy: 1.0000 - val_loss: 0.0094 - learning_rate: 5.0000e-05\n",
            "Epoch 20/20\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 14ms/step - accuracy: 0.9997 - loss: 0.0126 - val_accuracy: 1.0000 - val_loss: 0.0116 - learning_rate: 5.0000e-05\n",
            "Model training complete!\n",
            "Evaluating the model...\n",
            "\u001b[1m397/397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00      3418\n",
            "         1.0       1.00      1.00      1.00     21943\n",
            "\n",
            "    accuracy                           1.00     25361\n",
            "   macro avg       1.00      1.00      1.00     25361\n",
            "weighted avg       1.00      1.00      1.00     25361\n",
            "\n",
            "Accuracy: 0.7668\n",
            "AUC-ROC: 1.0000\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "from tensorflow.keras.mixed_precision import Policy, set_global_policy\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "# Enable mixed precision\n",
        "set_global_policy(Policy('mixed_float16'))\n",
        "\n",
        "# GPU Configuration: Set memory growth and limit\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if physical_devices:\n",
        "    try:\n",
        "        for device in physical_devices:\n",
        "            tf.config.experimental.set_memory_growth(device, True)\n",
        "        print(\"GPU memory growth enabled.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error configuring GPU: {e}\")\n",
        "else:\n",
        "    print(\"No GPU detected, running on CPU.\")\n",
        "\n",
        "# File paths\n",
        "FAKE_TRAIN_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/standardized_fake_train.pkl'\n",
        "REAL_TRAIN_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/standardized_real_train.pkl'\n",
        "FAKE_VALID_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/spatial_valid_fake.pkl'\n",
        "REAL_VALID_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/spatial_valid_real.pkl'\n",
        "CHECKPOINT_PATH = \"drive/MyDrive/SP_cup/checkpoints/model_optimized.keras\"\n",
        "os.makedirs(os.path.dirname(CHECKPOINT_PATH), exist_ok=True)\n",
        "\n",
        "# Function to load features\n",
        "def load_features(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "# Parallel feature validation and extraction\n",
        "def validate_and_extract(features):\n",
        "    def extract_feature(entry):\n",
        "        if isinstance(entry, list):\n",
        "            return [sub_entry['features'] for sub_entry in entry if isinstance(sub_entry, dict) and 'features' in sub_entry]\n",
        "        elif isinstance(entry, dict) and 'features' in entry:\n",
        "            return [entry['features']]\n",
        "        return []\n",
        "\n",
        "    with Parallel(n_jobs=-1, backend='threading') as parallel:\n",
        "        results = parallel(delayed(extract_feature)(entry) for entry in features)\n",
        "    valid_features = [item for sublist in results for item in sublist]\n",
        "    return np.array(valid_features, dtype=np.float32)\n",
        "\n",
        "# Augment features for robustness\n",
        "def augment_features(X, y, augment_factor=2):\n",
        "    augmented_X, augmented_y = [], []\n",
        "    for _ in range(augment_factor):\n",
        "        noise = np.random.normal(0, 0.01, X.shape)\n",
        "        scale = np.random.uniform(0.9, 1.1, X.shape)\n",
        "        X_augmented = X + noise\n",
        "        X_augmented *= scale\n",
        "        augmented_X.append(X_augmented)\n",
        "        augmented_y.append(y)\n",
        "    return np.vstack(augmented_X), np.hstack(augmented_y)\n",
        "\n",
        "# Build the optimized model\n",
        "def build_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Input(shape=input_shape),\n",
        "        Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.4),\n",
        "        Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Train and evaluate the model\n",
        "def train_and_evaluate():\n",
        "    print(\"Loading training features...\")\n",
        "\n",
        "    # Load features\n",
        "    fake_features = load_features(FAKE_TRAIN_FEATURES_PATH)\n",
        "    real_features = load_features(REAL_TRAIN_FEATURES_PATH)\n",
        "\n",
        "    # Validate and extract feature vectors\n",
        "    X_fake = validate_and_extract(fake_features)\n",
        "    X_real = validate_and_extract(real_features)\n",
        "\n",
        "    # Create labels\n",
        "    y_fake = np.ones(len(X_fake))\n",
        "    y_real = np.zeros(len(X_real))\n",
        "\n",
        "    # Combine data and labels\n",
        "    X_combined = np.vstack((X_fake, X_real))\n",
        "    y_combined = np.hstack((y_fake, y_real))\n",
        "\n",
        "    # Normalize features\n",
        "    scaler = StandardScaler()\n",
        "    X_combined = scaler.fit_transform(X_combined)\n",
        "\n",
        "    # Split into training and validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_combined, y_combined, test_size=0.2, random_state=42, stratify=y_combined\n",
        "    )\n",
        "\n",
        "    # Augment training data\n",
        "    print(\"Augmenting training data...\")\n",
        "    X_train, y_train = augment_features(X_train, y_train, augment_factor=2)\n",
        "\n",
        "    # Debugging shapes\n",
        "    print(\"Shape of X_train:\", X_train.shape)\n",
        "    print(\"Shape of X_val:\", X_val.shape)\n",
        "    print(\"Shape of y_train:\", y_train.shape)\n",
        "    print(\"Shape of y_val:\", y_val.shape)\n",
        "\n",
        "    # Build the model\n",
        "    input_shape = (X_train.shape[1],)\n",
        "    model = build_model(input_shape)\n",
        "\n",
        "    # Callbacks\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, mode='min'),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1),\n",
        "        ModelCheckpoint(CHECKPOINT_PATH, save_best_only=True, monitor='val_loss', mode='min')\n",
        "    ]\n",
        "\n",
        "    # Train the model\n",
        "    print(\"Starting training...\")\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=20,\n",
        "        batch_size=64,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1,\n",
        "        class_weight={0: 1.0, 1: 3.0}  # Adjust class weights as needed\n",
        "    )\n",
        "\n",
        "    print(\"Model training complete!\")\n",
        "\n",
        "    # After training, load the best model\n",
        "    model = load_model(CHECKPOINT_PATH)\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    print(\"Evaluating the model...\")\n",
        "    val_predictions = model.predict(X_val, batch_size=64, verbose=1)\n",
        "    val_predictions = (val_predictions > 0.5).astype(int)  # Threshold at 0.5 for binary classification\n",
        "\n",
        "    accuracy = np.mean(val_predictions == y_val)\n",
        "    auc = roc_auc_score(y_val, val_predictions)\n",
        "\n",
        "    # Classification report\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_val, val_predictions))\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"AUC-ROC: {auc:.4f}\")\n",
        "\n",
        "# Run training and evaluation\n",
        "train_and_evaluate()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OePmWCUebJCX",
        "outputId": "2580f668-f173-4af9-eac1-91e5f21cf341"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load validation features\n",
        "def load_validation_data():\n",
        "    fake_features = load_features(FAKE_VALID_FEATURES_PATH)\n",
        "    real_features = load_features(REAL_VALID_FEATURES_PATH)\n",
        "\n",
        "    # Extract feature vectors\n",
        "    X_fake = validate_and_extract(fake_features)\n",
        "    X_real = validate_and_extract(real_features)\n",
        "\n",
        "    # Create labels\n",
        "    y_fake = np.ones(len(X_fake))\n",
        "    y_real = np.zeros(len(X_real))\n",
        "\n",
        "    # Combine data and labels\n",
        "    X_combined = np.vstack((X_fake, X_real))\n",
        "    y_combined = np.hstack((y_fake, y_real))\n",
        "\n",
        "    # Fit and apply a new scaler\n",
        "    print(\"Fitting a new scaler...\")\n",
        "    scaler = StandardScaler()\n",
        "    X_combined = scaler.fit_transform(X_combined)\n",
        "\n",
        "    return X_combined, y_combined\n"
      ],
      "metadata": {
        "id": "6Z673UxxXdIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "import pickle\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "import joblib  # For saving and loading the scaler\n",
        "import os  # For checking file existence\n",
        "\n",
        "# File paths for validation data and scaler\n",
        "FAKE_VALID_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/spatial_valid_fake.pkl'\n",
        "REAL_VALID_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/spatial_valid_real.pkl'\n",
        "CHECKPOINT_PATH = \"drive/MyDrive/SP_cup/checkpoints/model_optimized.keras\"\n",
        "SCALER_PATH = 'drive/MyDrive/SP_cup/scaler.pkl'  # Path to save/load the scaler\n",
        "\n",
        "# Function to load features\n",
        "def load_features(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "# Parallel feature validation and extraction\n",
        "def validate_and_extract(features):\n",
        "    def extract_feature(entry):\n",
        "        if isinstance(entry, list):\n",
        "            return [sub_entry['features'] for sub_entry in entry if isinstance(sub_entry, dict) and 'features' in sub_entry]\n",
        "        elif isinstance(entry, dict) and 'features' in entry:\n",
        "            return [entry['features']]\n",
        "        return []\n",
        "\n",
        "    valid_features = [item for sublist in features for item in extract_feature(sublist)]\n",
        "    return np.array(valid_features, dtype=np.float32)\n",
        "\n",
        "# Load validation features\n",
        "def load_validation_data():\n",
        "    fake_features = load_features(FAKE_VALID_FEATURES_PATH)\n",
        "    real_features = load_features(REAL_VALID_FEATURES_PATH)\n",
        "\n",
        "    # Extract feature vectors\n",
        "    X_fake = validate_and_extract(fake_features)\n",
        "    X_real = validate_and_extract(real_features)\n",
        "\n",
        "    # Create labels\n",
        "    y_fake = np.ones(len(X_fake))\n",
        "    y_real = np.zeros(len(X_real))\n",
        "\n",
        "    # Combine data and labels\n",
        "    X_combined = np.vstack((X_fake, X_real))\n",
        "    y_combined = np.hstack((y_fake, y_real))\n",
        "\n",
        "    # Check if the scaler file exists\n",
        "    if os.path.exists(SCALER_PATH):\n",
        "        print(f\"Loading existing scaler from {SCALER_PATH}...\")\n",
        "        scaler = joblib.load(SCALER_PATH)\n",
        "    else:\n",
        "        print(f\"Scaler not found at {SCALER_PATH}. Creating and saving a new scaler...\")\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(X_combined)\n",
        "        joblib.dump(scaler, SCALER_PATH)\n",
        "        print(f\"Scaler saved at {SCALER_PATH}.\")\n",
        "\n",
        "    # Normalize features\n",
        "    X_combined = scaler.transform(X_combined)\n",
        "\n",
        "    return X_combined, y_combined\n",
        "\n",
        "# Test the model\n",
        "def test_model():\n",
        "    print(\"Loading validation data...\")\n",
        "    X_val, y_val = load_validation_data()\n",
        "\n",
        "    # Load the best model\n",
        "    model = load_model(CHECKPOINT_PATH)\n",
        "\n",
        "    # Evaluate on the validation set\n",
        "    print(\"Evaluating the model...\")\n",
        "    val_predictions = model.predict(X_val, batch_size=64, verbose=1)\n",
        "    val_predictions = (val_predictions > 0.5).astype(int)  # Threshold at 0.5 for binary classification\n",
        "\n",
        "    accuracy = np.mean(val_predictions == y_val)\n",
        "    auc = roc_auc_score(y_val, val_predictions)\n",
        "\n",
        "    # Classification report\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_val, val_predictions))\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"AUC-ROC: {auc:.4f}\")\n",
        "\n",
        "# Run testing\n",
        "test_model()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVqr02UKepqH",
        "outputId": "182b2ebe-576d-4873-9d7d-23a8b7985373"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading validation data...\n",
            "Scaler not found at drive/MyDrive/SP_cup/scaler.pkl. Creating and saving a new scaler...\n",
            "Scaler saved at drive/MyDrive/SP_cup/scaler.pkl.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 30 variables whereas the saved optimizer has 34 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating the model...\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.31      0.00      0.01      1548\n",
            "         1.0       0.50      0.99      0.66      1524\n",
            "\n",
            "    accuracy                           0.49      3072\n",
            "   macro avg       0.40      0.50      0.33      3072\n",
            "weighted avg       0.40      0.49      0.33      3072\n",
            "\n",
            "Accuracy: 0.4961\n",
            "AUC-ROC: 0.4980\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-JeOf9GoXfJy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}