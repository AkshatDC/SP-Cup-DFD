{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"16XHdIkwpuKSi6HPLAT4rLk_V8ale9a22","authorship_tag":"ABX9TyNDtQBZQK1kXCma9YNvCiPy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l6jNKMxvisl4","executionInfo":{"status":"ok","timestamp":1737132814587,"user_tz":-330,"elapsed":27426,"user":{"displayName":"Deepfake Detection","userId":"16853363008574283880"}},"outputId":"e8cc2c7e-e5ad-41a7-8c84-6690ec7ef22c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!pip install catboost"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d4d6AFcni__z","executionInfo":{"status":"ok","timestamp":1737132821197,"user_tz":-330,"elapsed":6651,"user":{"displayName":"Deepfake Detection","userId":"16853363008574283880"}},"outputId":"122a9195-9657-48c4-c36f-c0cca4f2168c"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting catboost\n","  Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (1.26.4)\n","Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.13.1)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2024.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.55.3)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.1)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.0.0)\n","Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl (98.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: catboost\n","Successfully installed catboost-1.2.7\n"]}]},{"cell_type":"code","source":["!pip install scikit-optimize"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RrxvWiJVjJUH","executionInfo":{"status":"ok","timestamp":1737132823075,"user_tz":-330,"elapsed":1895,"user":{"displayName":"Deepfake Detection","userId":"16853363008574283880"}},"outputId":"c820fb4a-4d7f-48d5-ed06-252df546c68d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting scikit-optimize\n","  Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl.metadata (9.7 kB)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.4.2)\n","Collecting pyaml>=16.9 (from scikit-optimize)\n","  Downloading pyaml-25.1.0-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.26.4)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.13.1)\n","Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.6.0)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (24.2)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.5.0)\n","Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl (107 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.8/107.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyaml-25.1.0-py3-none-any.whl (26 kB)\n","Installing collected packages: pyaml, scikit-optimize\n","Successfully installed pyaml-25.1.0 scikit-optimize-0.10.2\n"]}]},{"cell_type":"code","source":["pip install --upgrade scikit-learn xgboost"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kwXuxf3towbt","executionInfo":{"status":"ok","timestamp":1737132830056,"user_tz":-330,"elapsed":6995,"user":{"displayName":"Deepfake Detection","userId":"16853363008574283880"}},"outputId":"36ba29ff-71cb-42a9-81ef-31c298edfa0f"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.0)\n","Collecting scikit-learn\n","  Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n","Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.3)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n","Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n","Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: scikit-learn\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.6.0\n","    Uninstalling scikit-learn-1.6.0:\n","      Successfully uninstalled scikit-learn-1.6.0\n","Successfully installed scikit-learn-1.6.1\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4uqrhLldiguX","outputId":"1fbd8375-7988-48a8-9a59-1aa535a4d920"},"outputs":[{"output_type":"stream","name":"stdout","text":["Configured GPU with memory growth and 11 GB limit.\n","Loading training features...\n","Augmenting training data...\n"]}],"source":["import numpy as np\n","import pickle\n","import os\n","import tensorflow as tf\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, roc_auc_score\n","from sklearn.preprocessing import StandardScaler\n","from catboost import CatBoostClassifier\n","from sklearn.ensemble import VotingClassifier\n","from skopt import BayesSearchCV\n","\n","# GPU Configuration: Set memory growth and limit to 11 GB\n","physical_devices = tf.config.list_physical_devices('GPU')\n","if physical_devices:\n","    try:\n","        for device in physical_devices:\n","            tf.config.experimental.set_memory_growth(device, True)\n","            tf.config.experimental.VirtualDeviceConfiguration(\n","                device,\n","                [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=11264)]  # Limit to 11 GB\n","            )\n","        print(\"Configured GPU with memory growth and 11 GB limit.\")\n","    except Exception as e:\n","        print(f\"Error configuring GPU: {e}\")\n","else:\n","    print(\"No GPU detected, running on CPU.\")\n","\n","# File paths\n","FAKE_TRAIN_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/standardized_fake_train.pkl'\n","REAL_TRAIN_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/standardized_real_train.pkl'\n","FAKE_VALID_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/spatial_valid_fake.pkl'\n","REAL_VALID_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/spatial_valid_real.pkl'\n","CHECKPOINT_PATH = \"drive/MyDrive/SP_cup/checkpoints/ensemble_model.pkl\"\n","os.makedirs(os.path.dirname(CHECKPOINT_PATH), exist_ok=True)\n","\n","# Function to load features\n","def load_features(file_path):\n","    with open(file_path, 'rb') as f:\n","        return pickle.load(f)\n","\n","# Validate and extract feature vectors\n","def validate_and_extract(features):\n","    def extract_feature(entry):\n","        if isinstance(entry, list):\n","            return [sub_entry['features'] for sub_entry in entry if isinstance(sub_entry, dict) and 'features' in sub_entry]\n","        elif isinstance(entry, dict) and 'features' in entry:\n","            return [entry['features']]\n","        return []\n","\n","    valid_features = []\n","    for entry in features:\n","        valid_features.extend(extract_feature(entry))\n","    return np.array(valid_features, dtype=np.float32)\n","\n","# Augment features for robustness\n","def augment_features(X, y, augment_factor=1):\n","    augmented_X, augmented_y = [], []\n","    for _ in range(augment_factor):\n","        noise = np.random.normal(0, 0.01, X.shape)\n","        scale = np.random.uniform(0.9, 1.1, X.shape)\n","        X_augmented = X + noise\n","        X_augmented *= scale\n","        augmented_X.append(X_augmented)\n","        augmented_y.append(y)\n","    return np.vstack(augmented_X), np.hstack(augmented_y)\n","\n","# Function for Bayesian Optimization with memory-safe defaults\n","def optimize_model(model, param_grid, X_train, y_train, n_iter=5):\n","    try:\n","        bayes_search = BayesSearchCV(\n","            model,\n","            param_grid,\n","            n_iter=n_iter,\n","            cv=3,\n","            scoring='roc_auc',\n","            n_jobs=1,  # Reduce to 1 to avoid memory overload\n","            verbose=1\n","        )\n","        bayes_search.fit(X_train, y_train)\n","        return bayes_search.best_estimator_\n","    except Exception as e:\n","        print(f\"Optimization failed for {model.__class__.__name__}: {e}\")\n","        return None\n","\n","# Sequential optimization\n","def optimize_models_sequentially(X_train, y_train):\n","    results = {}\n","\n","    # Optimizing CatBoost\n","    print(\"Optimizing CatBoost...\")\n","    try:\n","        catboost_params = {\n","            'depth': (4, 6),  # Narrow range for fewer resources\n","            'learning_rate': (1e-3, 0.05, 'log-uniform'),\n","            'iterations': (50, 150)  # Reduced iterations\n","        }\n","        results['catboost'] = optimize_model(\n","            CatBoostClassifier(verbose=0, task_type='GPU'),\n","            catboost_params,\n","            X_train,\n","            y_train\n","        )\n","    except Exception as e:\n","        print(f\"CatBoost optimization failed: {e}\")\n","\n","    # Ensure at least one model succeeds\n","    if not results['catboost']:\n","        raise ValueError(\"Optimization incomplete: All models failed.\")\n","\n","    return {k: v for k, v in results.items() if v is not None}\n","\n","# Train and evaluate the ensemble model\n","def train_and_evaluate():\n","    print(\"Loading training features...\")\n","\n","    # Load features\n","    fake_features = load_features(FAKE_TRAIN_FEATURES_PATH)\n","    real_features = load_features(REAL_TRAIN_FEATURES_PATH)\n","\n","    # Validate and extract feature vectors\n","    X_fake = validate_and_extract(fake_features)\n","    X_real = validate_and_extract(real_features)\n","\n","    # Create labels\n","    y_fake = np.ones(len(X_fake))\n","    y_real = np.zeros(len(X_real))\n","\n","    # Combine data and labels\n","    X_combined = np.vstack((X_fake, X_real))\n","    y_combined = np.hstack((y_fake, y_real))\n","\n","    # Normalize features\n","    scaler = StandardScaler()\n","    X_combined = scaler.fit_transform(X_combined)\n","\n","    # Split into training and validation sets\n","    X_train, X_val, y_train, y_val = train_test_split(\n","        X_combined, y_combined, test_size=0.2, random_state=42, stratify=y_combined\n","    )\n","\n","    # Augment training data\n","    print(\"Augmenting training data...\")\n","    X_train, y_train = augment_features(X_train, y_train, augment_factor=1)  # Reduced factor\n","\n","    # Debugging shapes\n","    print(\"Shape of X_train:\", X_train.shape)\n","    print(\"Shape of X_val:\", X_val.shape)\n","    print(\"Shape of y_train:\", y_train.shape)\n","    print(\"Shape of y_val:\", y_val.shape)\n","\n","    # Sequential model optimization\n","    optimized_models = optimize_models_sequentially(X_train, y_train)\n","    catboost = optimized_models['catboost']\n","\n","    # Combine models into ensemble\n","    print(\"Creating ensemble model...\")\n","    ensemble = VotingClassifier(estimators=[\n","        ('catboost', catboost)\n","    ], voting='soft')\n","\n","    # Train ensemble\n","    print(\"Training ensemble model...\")\n","    for _ in tqdm(range(1), desc=\"Training Loop\"):\n","        ensemble.fit(X_train, y_train)\n","\n","    # Save ensemble model\n","    with open(CHECKPOINT_PATH, 'wb') as f:\n","        pickle.dump(ensemble, f)\n","\n","    # Evaluate on validation set\n","    print(\"Evaluating the ensemble model...\")\n","    val_predictions = ensemble.predict(X_val)\n","    val_probabilities = ensemble.predict_proba(X_val)[:, 1]\n","\n","    accuracy = np.mean(val_predictions == y_val)\n","    auc = roc_auc_score(y_val, val_probabilities)\n","\n","    # Classification report\n","    print(\"Classification Report:\")\n","    print(classification_report(y_val, val_predictions))\n","\n","    print(f\"Accuracy: {accuracy:.4f}\")\n","    print(f\"AUC-ROC: {auc:.4f}\")\n","\n","# Run training and evaluation\n","if __name__ == \"__main__\":\n","    train_and_evaluate()\n"]},{"cell_type":"code","source":[],"metadata":{"id":"sEI2hVxd-md9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pickle\n","import os\n","import tensorflow as tf\n","from tqdm import tqdm\n","from sklearn.metrics import classification_report, roc_auc_score, f1_score, precision_score, recall_score\n","from sklearn.preprocessing import StandardScaler\n","\n","# GPU Configuration: Set memory growth and limit to 11 GB\n","physical_devices = tf.config.list_physical_devices('GPU')\n","if physical_devices:\n","    try:\n","        for device in physical_devices:\n","            tf.config.experimental.set_memory_growth(device, True)\n","            tf.config.experimental.VirtualDeviceConfiguration(\n","                device,\n","                [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=11264)]  # Limit to 11 GB\n","            )\n","        print(\"Configured GPU with memory growth and 11 GB limit.\")\n","    except Exception as e:\n","        print(f\"Error configuring GPU: {e}\")\n","else:\n","    print(\"No GPU detected, running on CPU.\")\n","\n","# File paths\n","FAKE_VALID_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/spatial_valid_fake.pkl'\n","REAL_VALID_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/spatial_valid_real.pkl'\n","CHECKPOINT_PATH = \"drive/MyDrive/SP_cup/checkpoints/ensemble_model.pkl\"\n","\n","# Function to load features\n","def load_features(file_path):\n","    with open(file_path, 'rb') as f:\n","        return pickle.load(f)\n","\n","# Validate and extract feature vectors\n","def validate_and_extract(features):\n","    valid_features = []\n","    for entry in tqdm(features, desc=\"Validating and extracting features\"):\n","        if isinstance(entry, list):\n","            valid_features.extend(\n","                [sub_entry['features'] for sub_entry in entry if isinstance(sub_entry, dict) and 'features' in sub_entry]\n","            )\n","        elif isinstance(entry, dict) and 'features' in entry:\n","            valid_features.append(entry['features'])\n","    return np.array(valid_features, dtype=np.float32)\n","\n","# Function to evaluate the model\n","def evaluate_model():\n","    print(\"Loading validation features...\")\n","\n","    # Load validation features\n","    fake_features = load_features(FAKE_VALID_FEATURES_PATH)\n","    real_features = load_features(REAL_VALID_FEATURES_PATH)\n","\n","    # Validate and extract feature vectors\n","    X_fake = validate_and_extract(fake_features)\n","    X_real = validate_and_extract(real_features)\n","\n","    # Create labels\n","    y_fake = np.ones(len(X_fake))\n","    y_real = np.zeros(len(X_real))\n","\n","    # Combine data and labels\n","    X_val = np.vstack((X_fake, X_real))\n","    y_val = np.hstack((y_fake, y_real))\n","\n","    # Normalize features\n","    scaler = StandardScaler()\n","    X_val = scaler.fit_transform(X_val)\n","\n","    # Load the trained ensemble model\n","    print(\"Loading the trained model...\")\n","    with open(CHECKPOINT_PATH, 'rb') as f:\n","        ensemble = pickle.load(f)\n","\n","    # Make predictions\n","    print(\"Making predictions...\")\n","    val_predictions = ensemble.predict(X_val)\n","    val_probabilities = ensemble.predict_proba(X_val)[:, 1]\n","\n","    # Evaluate metrics\n","    accuracy = np.mean(val_predictions == y_val)\n","    auc = roc_auc_score(y_val, val_probabilities)\n","    f1 = f1_score(y_val, val_predictions)\n","    precision = precision_score(y_val, val_predictions)\n","    recall = recall_score(y_val, val_predictions)\n","\n","    # Print metrics\n","    print(\"Classification Report:\")\n","    print(classification_report(y_val, val_predictions))\n","    print(f\"Accuracy: {accuracy:.4f}\")\n","    print(f\"AUC-ROC: {auc:.4f}\")\n","    print(f\"F1 Score: {f1:.4f}\")\n","    print(f\"Precision: {precision:.4f}\")\n","    print(f\"Recall: {recall:.4f}\")\n","\n","# Run evaluation\n","if __name__ == \"__main__\":\n","    evaluate_model()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736935710234,"user_tz":-330,"elapsed":524,"user":{"displayName":"Deepfake Detection","userId":"16853363008574283880"}},"outputId":"52ee1510-329b-493e-c3be-4102f6979618","id":"9b-XFoZBTpvh"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Configured GPU with memory growth and 11 GB limit.\n","Loading validation features...\n"]},{"output_type":"stream","name":"stderr","text":["\n","Validating and extracting features: 100%|██████████| 1524/1524 [00:00<00:00, 770227.65it/s]\n","\n","Validating and extracting features: 100%|██████████| 1548/1548 [00:00<00:00, 821663.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loading the trained model...\n","Making predictions...\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.94      0.97      0.95      1548\n","         1.0       0.97      0.94      0.95      1524\n","\n","    accuracy                           0.95      3072\n","   macro avg       0.95      0.95      0.95      3072\n","weighted avg       0.95      0.95      0.95      3072\n","\n","Accuracy: 0.9538\n","AUC-ROC: 0.9933\n","F1 Score: 0.9525\n","Precision: 0.9707\n","Recall: 0.9350\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import pickle\n","import os\n","import tensorflow as tf\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, roc_auc_score\n","from sklearn.preprocessing import StandardScaler\n","from catboost import CatBoostClassifier\n","from xgboost import XGBClassifier\n","from sklearn.ensemble import VotingClassifier\n","from skopt import BayesSearchCV\n","from joblib import Parallel, delayed\n","from tensorflow.keras.applications import ResNet50\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Dense, Flatten, Dropout\n","\n","# GPU Configuration: Set memory growth and limit to 11 GB\n","physical_devices = tf.config.list_physical_devices('GPU')\n","if physical_devices:\n","    try:\n","        for device in physical_devices:\n","            tf.config.experimental.set_memory_growth(device, True)\n","        print(\"Configured GPU with memory growth and 11 GB limit.\")\n","    except Exception as e:\n","        print(f\"Error configuring GPU: {e}\")\n","else:\n","    print(\"No GPU detected, running on CPU.\")\n","\n","# File paths\n","FAKE_TRAIN_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/standardized_fake_train.pkl'\n","REAL_TRAIN_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/standardized_real_train.pkl'\n","FAKE_VALID_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/spatial_valid_fake.pkl'\n","REAL_VALID_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/spatial_valid_real.pkl'\n","CHECKPOINT_PATH = \"drive/MyDrive/SP_cup/checkpoints/ensemble_model.pkl\"\n","os.makedirs(os.path.dirname(CHECKPOINT_PATH), exist_ok=True)\n","\n","# Function to load features\n","def load_features(file_path):\n","    with open(file_path, 'rb') as f:\n","        return pickle.load(f)\n","\n","# Validate and extract feature vectors\n","def validate_and_extract(features):\n","    def extract_feature(entry):\n","        if isinstance(entry, list):\n","            return [sub_entry['features'] for sub_entry in entry if isinstance(sub_entry, dict) and 'features' in sub_entry]\n","        elif isinstance(entry, dict) and 'features' in entry:\n","            return [entry['features']]\n","        return []\n","\n","    valid_features = Parallel(n_jobs=-1)(\n","        delayed(extract_feature)(entry) for entry in tqdm(features, desc=\"Validating and extracting features\")\n","    )\n","    valid_features = [item for sublist in valid_features for item in sublist]  # Flatten list\n","    return np.array(valid_features, dtype=np.float32)\n","\n","# Augment features for robustness\n","def augment_features(X, y, augment_factor=1):\n","    augmented_X, augmented_y = [], []\n","    for _ in tqdm(range(augment_factor), desc=\"Augmenting features\"):\n","        noise = np.random.normal(0, 0.01, X.shape)\n","        scale = np.random.uniform(0.9, 1.1, X.shape)\n","        X_augmented = X + noise\n","        X_augmented *= scale\n","        augmented_X.append(X_augmented)\n","        augmented_y.append(y)\n","    return np.vstack(augmented_X), np.hstack(augmented_y)\n","\n","# Function for Bayesian Optimization with memory-safe defaults\n","def optimize_model(model, param_grid, X_train, y_train, n_iter=5):\n","    try:\n","        bayes_search = BayesSearchCV(\n","            model,\n","            param_grid,\n","            n_iter=n_iter,\n","            cv=3,\n","            scoring='roc_auc',\n","            n_jobs=-1,\n","            verbose=1\n","        )\n","        bayes_search.fit(X_train, y_train)\n","        return bayes_search.best_estimator_\n","    except Exception as e:\n","        print(f\"Optimization failed for {model.__class__.__name__}: {e}\")\n","        return None\n","\n","# Fix for XGBoost optimization\n","def optimize_xgboost(X_train, y_train):\n","    try:\n","        xgb_params = {\n","            'n_estimators': (50, 100),\n","            'learning_rate': (0.01, 0.2, 'log-uniform'),\n","            'max_depth': (3, 10),\n","            'colsample_bytree': (0.5, 1.0),\n","            'subsample': (0.5, 1.0)\n","        }\n","        xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","        best_xgb = optimize_model(xgb_model, xgb_params, X_train, y_train)\n","        return best_xgb\n","    except Exception as e:\n","        print(f\"XGBoost optimization failed: {e}\")\n","        return None\n","\n","# Updated function to optimize models sequentially\n","def optimize_models_sequentially(X_train, y_train):\n","    results = {}\n","\n","    # Optimizing CatBoost\n","    print(\"Optimizing CatBoost...\")\n","    try:\n","        catboost_params = {\n","            'depth': (4, 6),\n","            'learning_rate': (1e-3, 0.05, 'log-uniform'),\n","            'iterations': (50, 100)\n","        }\n","        results['catboost'] = optimize_model(\n","            CatBoostClassifier(verbose=0, task_type='CPU', thread_count=-1),\n","            catboost_params,\n","            X_train,\n","            y_train\n","        )\n","    except Exception as e:\n","        print(f\"CatBoost optimization failed: {e}\")\n","\n","    # Optimizing XGBoost\n","    print(\"Optimizing XGBoost...\")\n","    results['xgboost'] = optimize_xgboost(X_train, y_train)\n","\n","    # Ensure at least one model succeeds\n","    successful_models = {k: v for k, v in results.items() if v is not None}\n","    if not successful_models:\n","        raise ValueError(\"Optimization incomplete: All models failed.\")\n","\n","    return successful_models\n","\n","# Train and evaluate the ensemble model\n","def train_and_evaluate():\n","    print(\"Loading training features...\")\n","\n","    # Load features\n","    fake_features = load_features(FAKE_VALID_FEATURES_PATH)\n","    real_features = load_features(REAL_VALID_FEATURES_PATH)\n","\n","    # Validate and extract feature vectors\n","    X_fake = validate_and_extract(fake_features)\n","    X_real = validate_and_extract(real_features)\n","\n","    # Create labels\n","    y_fake = np.ones(len(X_fake))\n","    y_real = np.zeros(len(X_real))\n","\n","    # Combine data and labels\n","    X_combined = np.vstack((X_fake, X_real))\n","    y_combined = np.hstack((y_fake, y_real))\n","\n","    # Normalize features\n","    scaler = StandardScaler()\n","    X_combined = scaler.fit_transform(X_combined)\n","\n","    # Split into training and validation sets\n","    X_train, X_val, y_train, y_val = train_test_split(\n","        X_combined, y_combined, test_size=0.2, random_state=52, stratify=y_combined\n","    )\n","\n","    # Augment training data\n","    print(\"Augmenting training data...\")\n","    X_train, y_train = augment_features(X_train, y_train, augment_factor=1)\n","\n","    # Debugging shapes\n","    print(\"Shape of X_train:\", X_train.shape)\n","    print(\"Shape of X_val:\", X_val.shape)\n","    print(\"Shape of y_train:\", y_train.shape)\n","    print(\"Shape of y_val:\", y_val.shape)\n","\n","    # Load and configure ResNet-50 model\n","    print(\"Training ResNet-50 model...\")\n","    base_model = ResNet50(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n","    for layer in base_model.layers:\n","        layer.trainable = False\n","\n","    resnet_model = tf.keras.Sequential([\n","        base_model,\n","        Flatten(),\n","        Dense(256, activation='relu'),\n","        Dropout(0.3),\n","        Dense(1, activation='sigmoid')\n","    ])\n","\n","    resnet_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","    # Example: Adjust X_train for ResNet-50 (if image data is used)\n","    # X_train_resized = resize_images(X_train)\n","    # resnet_model.fit(X_train_resized, y_train, epochs=5, batch_size=32, validation_data=(X_val, y_val))\n","\n","    # Sequential model optimization\n","    optimized_models = optimize_models_sequentially(X_train, y_train)\n","    catboost = optimized_models.get('catboost')\n","    xgboost = optimized_models.get('xgboost')\n","\n","    # Combine models into ensemble\n","    print(\"Creating ensemble model...\")\n","    ensemble_estimators = []\n","    if catboost:\n","        ensemble_estimators.append(('catboost', catboost))\n","    if xgboost:\n","        ensemble_estimators.append(('xgboost', xgboost))\n","\n","    ensemble = VotingClassifier(estimators=ensemble_estimators, voting='soft')\n","\n","    # Train ensemble\n","    print(\"Training ensemble model...\")\n","    for _ in tqdm(range(1), desc=\"Training Loop\"):\n","        ensemble.fit(X_train, y_train)\n","\n","    # Save ensemble model\n","    with open(CHECKPOINT_PATH, 'wb') as f:\n","        pickle.dump(ensemble, f)\n","\n","    # Evaluate on validation set\n","    print(\"Evaluating the ensemble model...\")\n","    val_predictions = ensemble.predict(X_val)\n","    val_probabilities = ensemble.predict_proba(X_val)[:, 1]\n","\n","    accuracy = np.mean(val_predictions == y_val)\n","    auc = roc_auc_score(y_val, val_probabilities)\n","\n","    # Classification report\n","    print(\"Classification Report:\")\n","    print(classification_report(y_val, val_predictions))\n","\n","    print(f\"Accuracy: {accuracy:.4f}\")\n","    print(f\"AUC-ROC: {auc:.4f}\")\n","\n","# Run training and evaluation\n","if __name__ == \"__main__\":\n","    train_and_evaluate()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"faLuH2H2TUMA","executionInfo":{"status":"ok","timestamp":1736935628103,"user_tz":-330,"elapsed":324585,"user":{"displayName":"Deepfake Detection","userId":"16853363008574283880"}},"outputId":"15f48383-0bf7-4dff-8dba-8aee53173886"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Configured GPU with memory growth and 11 GB limit.\n","Loading training features...\n"]},{"output_type":"stream","name":"stderr","text":["\n","Validating and extracting features:   0%|          | 0/1524 [00:00<?, ?it/s]\u001b[A\n","Validating and extracting features:   0%|          | 2/1524 [00:00<01:19, 19.06it/s]\u001b[A\n","Validating and extracting features:   0%|          | 4/1524 [00:00<02:24, 10.55it/s]\u001b[A\n","Validating and extracting features: 100%|██████████| 1524/1524 [00:00<00:00, 3237.10it/s]\n","\n","Validating and extracting features: 100%|██████████| 1548/1548 [00:00<00:00, 21309.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Augmenting training data...\n"]},{"output_type":"stream","name":"stderr","text":["\n","Augmenting features:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n","Augmenting features: 100%|██████████| 1/1 [00:00<00:00,  7.25it/s]"]},{"output_type":"stream","name":"stdout","text":["Shape of X_train: (2457, 1280)\n","Shape of X_val: (615, 1280)\n","Shape of y_train: (2457,)\n","Shape of y_val: (615,)\n","Training ResNet-50 model...\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Optimizing CatBoost...\n","Fitting 3 folds for each of 1 candidates, totalling 3 fits\n","Fitting 3 folds for each of 1 candidates, totalling 3 fits\n","Fitting 3 folds for each of 1 candidates, totalling 3 fits\n","Fitting 3 folds for each of 1 candidates, totalling 3 fits\n","Fitting 3 folds for each of 1 candidates, totalling 3 fits\n","Optimizing XGBoost...\n","Optimization failed for XGBClassifier: 'super' object has no attribute '__sklearn_tags__'\n","Creating ensemble model...\n","Training ensemble model...\n"]},{"output_type":"stream","name":"stderr","text":["\n","Training Loop:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n","Training Loop: 100%|██████████| 1/1 [00:27<00:00, 27.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Evaluating the ensemble model...\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.92      0.96      0.94       310\n","         1.0       0.96      0.91      0.93       305\n","\n","    accuracy                           0.94       615\n","   macro avg       0.94      0.94      0.94       615\n","weighted avg       0.94      0.94      0.94       615\n","\n","Accuracy: 0.9366\n","AUC-ROC: 0.9847\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import pickle\n","import os\n","import tensorflow as tf\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, roc_auc_score\n","from sklearn.preprocessing import StandardScaler\n","from catboost import CatBoostClassifier\n","from xgboost import XGBClassifier\n","from sklearn.ensemble import VotingClassifier\n","from skopt import BayesSearchCV\n","from imblearn.over_sampling import SMOTE\n","from joblib import Parallel, delayed\n","from multiprocessing import Pool, cpu_count\n","\n","# GPU Configuration\n","physical_devices = tf.config.list_physical_devices('GPU')\n","if physical_devices:\n","    try:\n","        for device in physical_devices:\n","            tf.config.experimental.set_memory_growth(device, True)\n","        print(\"Configured GPU with memory growth.\")\n","    except Exception as e:\n","        print(f\"Error configuring GPU: {e}\")\n","else:\n","    print(\"No GPU detected, running on CPU.\")\n","\n","# File paths\n","FAKE_TRAIN_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/standardized_fake_train.pkl'\n","REAL_TRAIN_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/standardized_real_train.pkl'\n","CHECKPOINT_PATH = \"drive/MyDrive/SP_cup/checkpoints/ensemble_model.pkl\"\n","os.makedirs(os.path.dirname(CHECKPOINT_PATH), exist_ok=True)\n","\n","# Function to load features\n","def load_features(file_path):\n","    with open(file_path, 'rb') as f:\n","        return pickle.load(f)\n","\n","# Extract features with validation\n","def validate_and_extract(features):\n","    with Pool(cpu_count()) as pool:\n","        valid_features = list(tqdm(pool.imap(lambda f: f.get('features', []) if isinstance(f, dict) else [], features),\n","                                   total=len(features), desc=\"Validating and extracting features\"))\n","    return np.array([item for sublist in valid_features for item in sublist], dtype=np.float32)\n","\n","# Augment features\n","def augment_features(X, y, augment_factor=1):\n","    augmented_X, augmented_y = [], []\n","    for _ in tqdm(range(augment_factor), desc=\"Augmenting features\"):\n","        noise = np.random.normal(0, 0.01, X.shape)\n","        scale = np.random.uniform(0.9, 1.1, X.shape)\n","        X_augmented = X + noise\n","        X_augmented *= scale\n","        augmented_X.append(X_augmented)\n","        augmented_y.append(y)\n","    return np.vstack(augmented_X), np.hstack(augmented_y)\n","\n","# Optimize models\n","def optimize_model(model, param_grid, X_train, y_train, n_iter=2):\n","    try:\n","        search = BayesSearchCV(\n","            model,\n","            param_grid,\n","            n_iter=n_iter,\n","            cv=3,\n","            scoring='roc_auc',\n","            n_jobs=-1,\n","            verbose=1\n","        )\n","        search.fit(X_train, y_train)\n","        return search.best_estimator_\n","    except Exception as e:\n","        print(f\"Optimization failed for {model.__class__.__name__}: {e}\")\n","        return None\n","\n","# Restart session\n","def restart_session():\n","    print(\"Restarting session to avoid crashes...\")\n","    os.kill(os.getpid(), 9)\n","\n","# Manage memory\n","def manage_memory():\n","    import gc\n","    gc.collect()\n","    tf.keras.backend.clear_session()\n","\n","# Train and evaluate ensemble model with session restart\n","def train_and_evaluate_with_restarts():\n","    try:\n","        print(\"Loading training features...\")\n","\n","        # Load features\n","        X_fake = validate_and_extract(load_features(FAKE_TRAIN_FEATURES_PATH))\n","        X_real = validate_and_extract(load_features(REAL_TRAIN_FEATURES_PATH))\n","\n","        # Create labels\n","        y_fake = np.ones(len(X_fake))\n","        y_real = np.zeros(len(X_real))\n","\n","        # Combine data and labels\n","        X_combined = np.vstack((X_fake, X_real))\n","        y_combined = np.hstack((y_fake, y_real))\n","\n","        # Normalize features\n","        scaler = StandardScaler()\n","        X_combined = scaler.fit_transform(X_combined)\n","\n","        # Split into training and validation sets\n","        X_train, X_val, y_train, y_val = train_test_split(\n","            X_combined, y_combined, test_size=0.2, random_state=52, stratify=y_combined\n","        )\n","\n","        # Apply SMOTE\n","        print(\"Applying SMOTE...\")\n","        smote = SMOTE(random_state=52)\n","        X_train, y_train = smote.fit_resample(X_train, y_train)\n","\n","        # Augment training data\n","        print(\"Augmenting training data...\")\n","        X_train, y_train = augment_features(X_train, y_train, augment_factor=1)\n","\n","        # Debug shapes\n","        print(f\"X_train shape: {X_train.shape}, X_val shape: {X_val.shape}\")\n","\n","        # Optimizing models\n","        print(\"Optimizing models...\")\n","        catboost_params = {'depth': (4, 6), 'learning_rate': (1e-3, 0.05, 'log-uniform'), 'iterations': (50, 100)}\n","        xgboost_params = {'n_estimators': (50, 100), 'learning_rate': (0.01, 0.2, 'log-uniform'), 'max_depth': (3, 10)}\n","\n","        catboost = optimize_model(CatBoostClassifier(verbose=0), catboost_params, X_train, y_train)\n","        xgboost = optimize_model(XGBClassifier(use_label_encoder=False, eval_metric='logloss'), xgboost_params, X_train, y_train)\n","\n","        # Create and train ensemble\n","        print(\"Training ensemble model...\")\n","        ensemble_estimators = []\n","        if catboost: ensemble_estimators.append(('catboost', catboost))\n","        if xgboost: ensemble_estimators.append(('xgboost', xgboost))\n","\n","        if not ensemble_estimators:\n","            raise ValueError(\"No models were successfully optimized.\")\n","\n","        ensemble = VotingClassifier(estimators=ensemble_estimators, voting='soft')\n","        ensemble.fit(X_train, y_train)\n","\n","        # Save model\n","        with open(CHECKPOINT_PATH, 'wb') as f:\n","            pickle.dump(ensemble, f)\n","\n","        # Evaluate model\n","        val_preds = ensemble.predict(X_val)\n","        val_probs = ensemble.predict_proba(X_val)[:, 1]\n","        print(classification_report(y_val, val_preds))\n","        print(f\"AUC-ROC: {roc_auc_score(y_val, val_probs):.4f}\")\n","\n","    except MemoryError:\n","        print(\"MemoryError detected! Restarting session...\")\n","        restart_session()\n","    except Exception as e:\n","        print(f\"Error occurred: {e}\")\n","        restart_session()\n","\n","# Main execution\n","if __name__ == \"__main__\":\n","    train_and_evaluate_with_restarts()\n"],"metadata":{"id":"9K9nL7RJdFv2","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b6814123-1e2a-40d4-8499-d898cbc4dfc8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Configured GPU with memory growth.\n","Loading training features...\n"]},{"output_type":"stream","name":"stderr","text":["Validating and extracting features:   0%|          | 0/16 [00:00<?, ?it/s]"]}]},{"cell_type":"code","source":[],"metadata":{"id":"StQ_MvJcEP5j"},"execution_count":null,"outputs":[]}]}