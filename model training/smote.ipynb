{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6jNKMxvisl4",
        "outputId": "251ba416-d503-4f7e-a3fe-06bf6c0622d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4d6AFcni__z",
        "outputId": "5aeaad97-9cf9-4d58-cb5b-1e3c8c87f123"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.0.0)\n",
            "Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-optimize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrxvWiJVjJUH",
        "outputId": "5145f91d-b164-40a8-8625-2d9b97c9b8e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-optimize\n",
            "  Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.4.2)\n",
            "Collecting pyaml>=16.9 (from scikit-optimize)\n",
            "  Downloading pyaml-25.1.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.6.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (24.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.5.0)\n",
            "Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.8/107.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyaml-25.1.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-25.1.0 scikit-optimize-0.10.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade scikit-learn xgboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwXuxf3towbt",
        "outputId": "c1bdddf2-52ff-4bab-ec2d-2e8570f10ffd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.3)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.0\n",
            "    Uninstalling scikit-learn-1.6.0:\n",
            "      Successfully uninstalled scikit-learn-1.6.0\n",
            "Successfully installed scikit-learn-1.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "4uqrhLldiguX",
        "outputId": "3b4f7a57-7073-4d4b-db6b-dc958fecc449"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configured GPU with memory growth.\n",
            "Loading training features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating and extracting features: 100%|██████████| 16/16 [00:01<00:00, 13.65it/s]\n",
            "Validating and extracting features: 100%|██████████| 17090/17090 [00:03<00:00, 5653.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_fake: (0,)\n",
            "Shape of X_real: (21875200,)\n",
            "Error: X_fake is empty. Check the feature extraction for FAKE_TRAIN_FEATURES_PATH.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "One or more feature sets are empty. Check input data and feature extraction logic.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-b811f7254732>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;31m# Main execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mtrain_and_evaluate_debug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-b811f7254732>\u001b[0m in \u001b[0;36mtrain_and_evaluate_debug\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;31m# Ensure no empty arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mX_fake\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mX_real\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"One or more feature sets are empty. Check input data and feature extraction logic.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# Create labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: One or more feature sets are empty. Check input data and feature extraction logic."
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from catboost import CatBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from multiprocessing import Pool, cpu_count\n",
        "\n",
        "# GPU Configuration\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if physical_devices:\n",
        "    try:\n",
        "        for device in physical_devices:\n",
        "            tf.config.experimental.set_memory_growth(device, True)\n",
        "        print(\"Configured GPU with memory growth.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error configuring GPU: {e}\")\n",
        "else:\n",
        "    print(\"No GPU detected, running on CPU.\")\n",
        "\n",
        "# File paths\n",
        "FAKE_TRAIN_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/standardized_fake_train.pkl'\n",
        "REAL_TRAIN_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/standardized_real_train.pkl'\n",
        "CHECKPOINT_PATH = \"drive/MyDrive/SP_cup/checkpoints/new_model.pkl\"\n",
        "os.makedirs(os.path.dirname(CHECKPOINT_PATH), exist_ok=True)\n",
        "\n",
        "# Function to load features\n",
        "def load_features(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "# Helper function for feature extraction\n",
        "def extract_features(entry):\n",
        "    if isinstance(entry, dict):\n",
        "        return entry.get('features', [])\n",
        "    return []\n",
        "\n",
        "# Extract features with validation\n",
        "def validate_and_extract(features):\n",
        "    with Pool(cpu_count()) as pool:\n",
        "        valid_features = list(tqdm(pool.imap(extract_features, features), total=len(features), desc=\"Validating and extracting features\"))\n",
        "    return np.array([item for sublist in valid_features for item in sublist], dtype=np.float32)\n",
        "\n",
        "# Augment features\n",
        "def augment_features(X, y, augment_factor=1):\n",
        "    augmented_X, augmented_y = [], []\n",
        "    for _ in tqdm(range(augment_factor), desc=\"Augmenting features\"):\n",
        "        noise = np.random.normal(0, 0.01, X.shape)\n",
        "        scale = np.random.uniform(0.9, 1.1, X.shape)\n",
        "        X_augmented = X + noise\n",
        "        X_augmented *= scale\n",
        "        augmented_X.append(X_augmented)\n",
        "        augmented_y.append(y)\n",
        "    return np.vstack(augmented_X), np.hstack(augmented_y)\n",
        "def debug_shapes(X_fake, X_real):\n",
        "    print(f\"Shape of X_fake: {X_fake.shape}\")\n",
        "    print(f\"Shape of X_real: {X_real.shape}\")\n",
        "    if X_fake.size == 0:\n",
        "        print(\"Error: X_fake is empty. Check the feature extraction for FAKE_TRAIN_FEATURES_PATH.\")\n",
        "    if X_real.size == 0:\n",
        "        print(\"Error: X_real is empty. Check the feature extraction for REAL_TRAIN_FEATURES_PATH.\")\n",
        "\n",
        "# Train and evaluate the model with debugging\n",
        "def train_and_evaluate_debug():\n",
        "    print(\"Loading training features...\")\n",
        "\n",
        "    # Load features\n",
        "    X_fake = validate_and_extract(load_features(FAKE_TRAIN_FEATURES_PATH))\n",
        "    X_real = validate_and_extract(load_features(REAL_TRAIN_FEATURES_PATH))\n",
        "\n",
        "    # Debug shapes of features\n",
        "    debug_shapes(X_fake, X_real)\n",
        "\n",
        "    # Ensure no empty arrays\n",
        "    if X_fake.size == 0 or X_real.size == 0:\n",
        "        raise ValueError(\"One or more feature sets are empty. Check input data and feature extraction logic.\")\n",
        "\n",
        "    # Create labels\n",
        "    y_fake = np.ones(len(X_fake))\n",
        "    y_real = np.zeros(len(X_real))\n",
        "\n",
        "    # Combine data and labels\n",
        "    X_combined = np.vstack((X_fake, X_real))\n",
        "    y_combined = np.hstack((y_fake, y_real))\n",
        "\n",
        "    # Normalize features\n",
        "    scaler = StandardScaler()\n",
        "    X_combined = scaler.fit_transform(X_combined)\n",
        "\n",
        "    # Split into training and validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_combined, y_combined, test_size=0.2, random_state=52, stratify=y_combined\n",
        "    )\n",
        "\n",
        "    # Apply SMOTE\n",
        "    print(\"Applying SMOTE...\")\n",
        "    smote = SMOTE(random_state=52, sampling_strategy=0.8)\n",
        "    X_train, y_train = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "    # Debug shapes\n",
        "    print(f\"X_train shape: {X_train.shape}, X_val shape: {X_val.shape}\")\n",
        "\n",
        "    # Initialize classifiers\n",
        "    catboost = CatBoostClassifier(verbose=0)\n",
        "\n",
        "    # Train classifier\n",
        "    print(\"Training CatBoostClassifier...\")\n",
        "    catboost.fit(X_train, y_train)\n",
        "\n",
        "    # Save model\n",
        "    with open(CHECKPOINT_PATH, 'wb') as f:\n",
        "        pickle.dump(catboost, f)\n",
        "\n",
        "    # Evaluate model\n",
        "    print(\"Evaluating model...\")\n",
        "    val_preds = catboost.predict(X_val)\n",
        "    val_probs = catboost.predict_proba(X_val)[:, 1]\n",
        "\n",
        "    print(classification_report(y_val, val_preds, zero_division=1))\n",
        "    print(f\"AUC-ROC: {roc_auc_score(y_val, val_probs):.4f}\")\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_evaluate_debug()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report, roc_auc_score, f1_score, precision_score, recall_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# GPU Configuration: Set memory growth and limit to 11 GB\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if physical_devices:\n",
        "    try:\n",
        "        for device in physical_devices:\n",
        "            tf.config.experimental.set_memory_growth(device, True)\n",
        "            tf.config.experimental.VirtualDeviceConfiguration(\n",
        "                device,\n",
        "                [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=11264)]  # Limit to 11 GB\n",
        "            )\n",
        "        print(\"Configured GPU with memory growth and 11 GB limit.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error configuring GPU: {e}\")\n",
        "else:\n",
        "    print(\"No GPU detected, running on CPU.\")\n",
        "\n",
        "# File paths\n",
        "FAKE_VALID_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/spatial_valid_fake.pkl'\n",
        "REAL_VALID_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/spatial_valid_real.pkl'\n",
        "CHECKPOINT_PATH = \"drive/MyDrive/SP_cup/checkpoints/ensemble_model.pkl\"\n",
        "\n",
        "# Function to load features\n",
        "def load_features(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "# Validate and extract feature vectors\n",
        "def validate_and_extract(features):\n",
        "    valid_features = []\n",
        "    for entry in tqdm(features, desc=\"Validating and extracting features\"):\n",
        "        if isinstance(entry, list):\n",
        "            valid_features.extend(\n",
        "                [sub_entry['features'] for sub_entry in entry if isinstance(sub_entry, dict) and 'features' in sub_entry]\n",
        "            )\n",
        "        elif isinstance(entry, dict) and 'features' in entry:\n",
        "            valid_features.append(entry['features'])\n",
        "    return np.array(valid_features, dtype=np.float32)\n",
        "\n",
        "# Function to evaluate the model\n",
        "def evaluate_model():\n",
        "    print(\"Loading validation features...\")\n",
        "\n",
        "    # Load validation features\n",
        "    fake_features = load_features(FAKE_VALID_FEATURES_PATH)\n",
        "    real_features = load_features(REAL_VALID_FEATURES_PATH)\n",
        "\n",
        "    # Validate and extract feature vectors\n",
        "    X_fake = validate_and_extract(fake_features)\n",
        "    X_real = validate_and_extract(real_features)\n",
        "\n",
        "    # Create labels\n",
        "    y_fake = np.ones(len(X_fake))\n",
        "    y_real = np.zeros(len(X_real))\n",
        "\n",
        "    # Combine data and labels\n",
        "    X_val = np.vstack((X_fake, X_real))\n",
        "    y_val = np.hstack((y_fake, y_real))\n",
        "\n",
        "    # Normalize features\n",
        "    scaler = StandardScaler()\n",
        "    X_val = scaler.fit_transform(X_val)\n",
        "\n",
        "    # Load the trained ensemble model\n",
        "    print(\"Loading the trained model...\")\n",
        "    with open(CHECKPOINT_PATH, 'rb') as f:\n",
        "        ensemble = pickle.load(f)\n",
        "\n",
        "    # Make predictions\n",
        "    print(\"Making predictions...\")\n",
        "    val_predictions = ensemble.predict(X_val)\n",
        "    val_probabilities = ensemble.predict_proba(X_val)[:, 1]\n",
        "\n",
        "    # Evaluate metrics\n",
        "    accuracy = np.mean(val_predictions == y_val)\n",
        "    auc = roc_auc_score(y_val, val_probabilities)\n",
        "    f1 = f1_score(y_val, val_predictions)\n",
        "    precision = precision_score(y_val, val_predictions)\n",
        "    recall = recall_score(y_val, val_predictions)\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_val, val_predictions))\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"AUC-ROC: {auc:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "\n",
        "# Run evaluation\n",
        "if __name__ == \"__main__\":\n",
        "    evaluate_model()\n"
      ],
      "metadata": {
        "id": "sEI2hVxd-md9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4897962-a40b-483d-e0db-feed3e388563"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configured GPU with memory growth and 11 GB limit.\n",
            "Loading validation features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating and extracting features: 100%|██████████| 1524/1524 [00:00<00:00, 1432568.20it/s]\n",
            "Validating and extracting features: 100%|██████████| 1548/1548 [00:00<00:00, 1024098.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the trained model...\n",
            "Making predictions...\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.00      0.00      0.00      1548\n",
            "         1.0       0.50      1.00      0.66      1524\n",
            "\n",
            "    accuracy                           0.50      3072\n",
            "   macro avg       0.25      0.50      0.33      3072\n",
            "weighted avg       0.25      0.50      0.33      3072\n",
            "\n",
            "Accuracy: 0.4961\n",
            "AUC-ROC: 0.5414\n",
            "F1 Score: 0.6632\n",
            "Precision: 0.4961\n",
            "Recall: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.6.0 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator VotingClassifier from version 1.6.0 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from catboost import CatBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from skopt import BayesSearchCV\n",
        "from joblib import Parallel, delayed\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "\n",
        "# GPU Configuration: Set memory growth and limit to 11 GB\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if physical_devices:\n",
        "    try:\n",
        "        for device in physical_devices:\n",
        "            tf.config.experimental.set_memory_growth(device, True)\n",
        "        print(\"Configured GPU with memory growth and 11 GB limit.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error configuring GPU: {e}\")\n",
        "else:\n",
        "    print(\"No GPU detected, running on CPU.\")\n",
        "\n",
        "# File paths\n",
        "FAKE_TRAIN_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/standardized_fake_train.pkl'\n",
        "REAL_TRAIN_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/standardized_real_train.pkl'\n",
        "FAKE_VALID_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/spatial_valid_fake.pkl'\n",
        "REAL_VALID_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/spatial_valid_real.pkl'\n",
        "CHECKPOINT_PATH = \"drive/MyDrive/SP_cup/checkpoints/ensemble_model.pkl\"\n",
        "os.makedirs(os.path.dirname(CHECKPOINT_PATH), exist_ok=True)\n",
        "\n",
        "# Function to load features\n",
        "def load_features(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "# Validate and extract feature vectors\n",
        "def validate_and_extract(features):\n",
        "    def extract_feature(entry):\n",
        "        if isinstance(entry, list):\n",
        "            return [sub_entry['features'] for sub_entry in entry if isinstance(sub_entry, dict) and 'features' in sub_entry]\n",
        "        elif isinstance(entry, dict) and 'features' in entry:\n",
        "            return [entry['features']]\n",
        "        return []\n",
        "\n",
        "    valid_features = Parallel(n_jobs=-1)(\n",
        "        delayed(extract_feature)(entry) for entry in tqdm(features, desc=\"Validating and extracting features\")\n",
        "    )\n",
        "    valid_features = [item for sublist in valid_features for item in sublist]  # Flatten list\n",
        "    return np.array(valid_features, dtype=np.float32)\n",
        "\n",
        "# Augment features for robustness\n",
        "def augment_features(X, y, augment_factor=1):\n",
        "    augmented_X, augmented_y = [], []\n",
        "    for _ in tqdm(range(augment_factor), desc=\"Augmenting features\"):\n",
        "        noise = np.random.normal(0, 0.01, X.shape)\n",
        "        scale = np.random.uniform(0.9, 1.1, X.shape)\n",
        "        X_augmented = X + noise\n",
        "        X_augmented *= scale\n",
        "        augmented_X.append(X_augmented)\n",
        "        augmented_y.append(y)\n",
        "    return np.vstack(augmented_X), np.hstack(augmented_y)\n",
        "\n",
        "# Function for Bayesian Optimization with memory-safe defaults\n",
        "def optimize_model(model, param_grid, X_train, y_train, n_iter=5):\n",
        "    try:\n",
        "        bayes_search = BayesSearchCV(\n",
        "            model,\n",
        "            param_grid,\n",
        "            n_iter=n_iter,\n",
        "            cv=3,\n",
        "            scoring='roc_auc',\n",
        "            n_jobs=-1,\n",
        "            verbose=1\n",
        "        )\n",
        "        bayes_search.fit(X_train, y_train)\n",
        "        return bayes_search.best_estimator_\n",
        "    except Exception as e:\n",
        "        print(f\"Optimization failed for {model.__class__.__name__}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Fix for XGBoost optimization\n",
        "def optimize_xgboost(X_train, y_train):\n",
        "    try:\n",
        "        xgb_params = {\n",
        "            'n_estimators': (50, 100),\n",
        "            'learning_rate': (0.01, 0.2, 'log-uniform'),\n",
        "            'max_depth': (3, 10),\n",
        "            'colsample_bytree': (0.5, 1.0),\n",
        "            'subsample': (0.5, 1.0)\n",
        "        }\n",
        "        xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "        best_xgb = optimize_model(xgb_model, xgb_params, X_train, y_train)\n",
        "        return best_xgb\n",
        "    except Exception as e:\n",
        "        print(f\"XGBoost optimization failed: {e}\")\n",
        "        return None\n",
        "\n",
        "# Updated function to optimize models sequentially\n",
        "def optimize_models_sequentially(X_train, y_train):\n",
        "    results = {}\n",
        "\n",
        "    # Optimizing CatBoost\n",
        "    print(\"Optimizing CatBoost...\")\n",
        "    try:\n",
        "        catboost_params = {\n",
        "            'depth': (4, 6),\n",
        "            'learning_rate': (1e-3, 0.05, 'log-uniform'),\n",
        "            'iterations': (50, 100)\n",
        "        }\n",
        "        results['catboost'] = optimize_model(\n",
        "            CatBoostClassifier(verbose=0, task_type='CPU', thread_count=-1),\n",
        "            catboost_params,\n",
        "            X_train,\n",
        "            y_train\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"CatBoost optimization failed: {e}\")\n",
        "\n",
        "    # Optimizing XGBoost\n",
        "    print(\"Optimizing XGBoost...\")\n",
        "    results['xgboost'] = optimize_xgboost(X_train, y_train)\n",
        "\n",
        "    # Ensure at least one model succeeds\n",
        "    successful_models = {k: v for k, v in results.items() if v is not None}\n",
        "    if not successful_models:\n",
        "        raise ValueError(\"Optimization incomplete: All models failed.\")\n",
        "\n",
        "    return successful_models\n",
        "\n",
        "# Train and evaluate the ensemble model\n",
        "def train_and_evaluate():\n",
        "    print(\"Loading training features...\")\n",
        "\n",
        "    # Load features\n",
        "    fake_features = load_features(FAKE_TRAIN_FEATURES_PATH)\n",
        "    real_features = load_features(REAL_TRAIN_FEATURES_PATH)\n",
        "\n",
        "    # Validate and extract feature vectors\n",
        "    X_fake = validate_and_extract(fake_features)\n",
        "    X_real = validate_and_extract(real_features)\n",
        "\n",
        "    # Create labels\n",
        "    y_fake = np.ones(len(X_fake))\n",
        "    y_real = np.zeros(len(X_real))\n",
        "\n",
        "    # Combine data and labels\n",
        "    X_combined = np.vstack((X_fake, X_real))\n",
        "    y_combined = np.hstack((y_fake, y_real))\n",
        "\n",
        "    # Normalize features\n",
        "    scaler = StandardScaler()\n",
        "    X_combined = scaler.fit_transform(X_combined)\n",
        "\n",
        "    # Split into training and validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_combined, y_combined, test_size=0.2, random_state=52, stratify=y_combined\n",
        "    )\n",
        "\n",
        "    # Augment training data\n",
        "    print(\"Augmenting training data...\")\n",
        "    X_train, y_train = augment_features(X_train, y_train, augment_factor=1)\n",
        "\n",
        "    # Debugging shapes\n",
        "    print(\"Shape of X_train:\", X_train.shape)\n",
        "    print(\"Shape of X_val:\", X_val.shape)\n",
        "    print(\"Shape of y_train:\", y_train.shape)\n",
        "    print(\"Shape of y_val:\", y_val.shape)\n",
        "\n",
        "    # Load and configure ResNet-50 model\n",
        "    print(\"Training ResNet-50 model...\")\n",
        "    base_model = ResNet50(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    resnet_model = tf.keras.Sequential([\n",
        "        base_model,\n",
        "        Flatten(),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    resnet_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    # Example: Adjust X_train for ResNet-50 (if image data is used)\n",
        "    # X_train_resized = resize_images(X_train)\n",
        "    # resnet_model.fit(X_train_resized, y_train, epochs=5, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "    # Sequential model optimization\n",
        "    optimized_models = optimize_models_sequentially(X_train, y_train)\n",
        "    catboost = optimized_models.get('catboost')\n",
        "    xgboost = optimized_models.get('xgboost')\n",
        "\n",
        "    # Combine models into ensemble\n",
        "    print(\"Creating ensemble model...\")\n",
        "    ensemble_estimators = []\n",
        "    if catboost:\n",
        "        ensemble_estimators.append(('catboost', catboost))\n",
        "    if xgboost:\n",
        "        ensemble_estimators.append(('xgboost', xgboost))\n",
        "\n",
        "    ensemble = VotingClassifier(estimators=ensemble_estimators, voting='soft')\n",
        "\n",
        "    # Train ensemble\n",
        "    print(\"Training ensemble model...\")\n",
        "    for _ in tqdm(range(1), desc=\"Training Loop\"):\n",
        "        ensemble.fit(X_train, y_train)\n",
        "\n",
        "    # Save ensemble model\n",
        "    with open(CHECKPOINT_PATH, 'wb') as f:\n",
        "        pickle.dump(ensemble, f)\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    print(\"Evaluating the ensemble model...\")\n",
        "    val_predictions = ensemble.predict(X_val)\n",
        "    val_probabilities = ensemble.predict_proba(X_val)[:, 1]\n",
        "\n",
        "    accuracy = np.mean(val_predictions == y_val)\n",
        "    auc = roc_auc_score(y_val, val_probabilities)\n",
        "\n",
        "    # Classification report\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_val, val_predictions))\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"AUC-ROC: {auc:.4f}\")\n",
        "\n",
        "# Run training and evaluation\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_evaluate()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faLuH2H2TUMA",
        "outputId": "1d8280e0-4cc7-44c5-d2d3-d1acf19fb137"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configured GPU with memory growth and 11 GB limit.\n",
            "Loading training features...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validating and extracting features:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Validating and extracting features:  12%|█▎        | 2/16 [00:00<00:00, 18.76it/s]\u001b[A\n",
            "Validating and extracting features:  25%|██▌       | 4/16 [00:01<00:05,  2.32it/s]\u001b[A\n",
            "Validating and extracting features:  38%|███▊      | 6/16 [00:02<00:04,  2.44it/s]\u001b[A\n",
            "Validating and extracting features:  50%|█████     | 8/16 [00:03<00:03,  2.50it/s]\u001b[A\n",
            "Validating and extracting features:  62%|██████▎   | 10/16 [00:03<00:02,  2.44it/s]\u001b[A\n",
            "Validating and extracting features:  75%|███████▌  | 12/16 [00:04<00:01,  2.23it/s]\u001b[A\n",
            "Validating and extracting features:  88%|████████▊ | 14/16 [00:06<00:01,  1.67it/s]\u001b[A\n",
            "Validating and extracting features: 100%|██████████| 16/16 [00:07<00:00,  2.05it/s]\n",
            "\n",
            "Validating and extracting features:   0%|          | 0/17090 [00:00<?, ?it/s]\u001b[A\n",
            "Validating and extracting features:  12%|█▏        | 2117/17090 [00:00<00:00, 21168.86it/s]\u001b[A\n",
            "Validating and extracting features:  36%|███▌      | 6140/17090 [00:00<00:00, 25658.14it/s]\u001b[A\n",
            "Validating and extracting features:  51%|█████     | 8649/17090 [00:00<00:00, 24563.48it/s]\u001b[A\n",
            "Validating and extracting features:  72%|███████▏  | 12284/17090 [00:00<00:00, 27126.40it/s]\u001b[A\n",
            "Validating and extracting features: 100%|██████████| 17090/17090 [00:00<00:00, 23006.76it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Augmenting training data...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Augmenting features:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Augmenting features: 100%|██████████| 1/1 [00:17<00:00, 17.28s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X_train: (101443, 1280)\n",
            "Shape of X_val: (25361, 1280)\n",
            "Shape of y_train: (101443,)\n",
            "Shape of y_val: (25361,)\n",
            "Training deep learning model...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 3ms/step - accuracy: 0.9970 - loss: 0.0051 - val_accuracy: 0.9988 - val_loss: 0.0217\n",
            "Epoch 2/10\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.9997 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 3.2321e-04\n",
            "Epoch 3/10\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 3.6690e-10 - val_accuracy: 1.0000 - val_loss: 3.2514e-04\n",
            "Epoch 4/10\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 2.0417e-06 - val_accuracy: 0.9999 - val_loss: 0.0014\n",
            "Epoch 5/10\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 1.1580e-05 - val_accuracy: 1.0000 - val_loss: 9.3084e-04\n",
            "Epoch 6/10\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 5.9939e-07 - val_accuracy: 1.0000 - val_loss: 6.4460e-04\n",
            "Epoch 7/10\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 1.1598e-10 - val_accuracy: 1.0000 - val_loss: 8.0451e-04\n",
            "Epoch 8/10\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 1.8710e-06 - val_accuracy: 1.0000 - val_loss: 0.0013\n",
            "Epoch 9/10\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.9999 - loss: 7.3470e-04 - val_accuracy: 1.0000 - val_loss: 3.6092e-32\n",
            "Epoch 10/10\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 3.5764e-04 - val_accuracy: 1.0000 - val_loss: 3.3396e-20\n",
            "Optimizing CatBoost...\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_tags.py:354: FutureWarning: The XGBClassifier or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimizing XGBoost...\n",
            "Optimization failed for XGBClassifier: 'super' object has no attribute '__sklearn_tags__'\n",
            "Creating ensemble model...\n",
            "Training ensemble model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Training Loop:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Training Loop: 100%|██████████| 1/1 [02:09<00:00, 129.97s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating the ensemble model...\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00      3418\n",
            "         1.0       1.00      1.00      1.00     21943\n",
            "\n",
            "    accuracy                           1.00     25361\n",
            "   macro avg       1.00      1.00      1.00     25361\n",
            "weighted avg       1.00      1.00      1.00     25361\n",
            "\n",
            "Accuracy: 0.9997\n",
            "AUC-ROC: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# File paths\n",
        "FAKE_VALID_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/spatial_valid_fake.pkl'\n",
        "REAL_VALID_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/spatial_valid_real.pkl'\n",
        "CHECKPOINT_PATH = \"drive/MyDrive/SP_cup/checkpoints/ensemble_model.pkl\"\n",
        "OUTPUT_SCORES_PATH = \"drive/MyDrive/SP_cup/results/validation_scores.txt\"\n",
        "\n",
        "# Function to load features\n",
        "def load_features(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "# Validate and extract feature vectors and file IDs\n",
        "def validate_and_extract(features, label):\n",
        "    valid_features = []\n",
        "    valid_file_ids = []\n",
        "    labels = []\n",
        "\n",
        "    for entry in tqdm(features, desc=\"Validating and extracting features\"):\n",
        "        if isinstance(entry, dict) and 'features' in entry and 'image_name' in entry:\n",
        "            valid_features.append(entry['features'])\n",
        "            # Extract the fileID from the 'image_name' field\n",
        "            file_id = os.path.basename(entry['image_name'])\n",
        "            valid_file_ids.append(file_id)\n",
        "            # Assign the label (1 for real, 0 for fake)\n",
        "            labels.append(label)\n",
        "\n",
        "    return np.array(valid_features, dtype=np.float32), valid_file_ids, labels\n",
        "\n",
        "# Generate evaluation set scores\n",
        "def generate_scores():\n",
        "    print(\"Loading validation features...\")\n",
        "\n",
        "    # Load validation features\n",
        "    fake_features = load_features(FAKE_VALID_FEATURES_PATH)\n",
        "    real_features = load_features(REAL_VALID_FEATURES_PATH)\n",
        "\n",
        "    # Extract features, file IDs, and labels\n",
        "    X_fake, fake_file_ids, y_fake = validate_and_extract(fake_features, label=0)\n",
        "    X_real, real_file_ids, y_real = validate_and_extract(real_features, label=1)\n",
        "\n",
        "    # Combine data and labels\n",
        "    X_val = np.vstack((X_fake, X_real))\n",
        "    y_val = np.hstack((y_fake, y_real))\n",
        "    file_ids = fake_file_ids + real_file_ids\n",
        "\n",
        "    # Normalize features\n",
        "    scaler = StandardScaler()\n",
        "    X_val = scaler.fit_transform(X_val)\n",
        "\n",
        "    # Load the trained ensemble model\n",
        "    print(\"Loading the trained model...\")\n",
        "    with open(CHECKPOINT_PATH, 'rb') as f:\n",
        "        ensemble = pickle.load(f)\n",
        "\n",
        "    # Generate probabilities (scores)\n",
        "    print(\"Generating evaluation scores...\")\n",
        "    val_probabilities = ensemble.predict_proba(X_val)[:, 1]\n",
        "\n",
        "    # Adjust scores to ensure real images have higher values and fake images lower\n",
        "    scores = [prob if label == 1 else 1 - prob for prob, label in zip(val_probabilities, y_val)]\n",
        "\n",
        "    # Save scores to the output file\n",
        "    print(f\"Saving scores to {OUTPUT_SCORES_PATH}...\")\n",
        "    with open(OUTPUT_SCORES_PATH, 'w') as f:\n",
        "        for file_id, score in zip(file_ids, scores):\n",
        "            f.write(f\"{file_id}\\t{score:.6f}\\n\")\n",
        "\n",
        "    print(\"Scores saved successfully.\")\n",
        "\n",
        "# Run score generation\n",
        "if __name__ == \"__main__\":\n",
        "    generate_scores()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9K9nL7RJdFv2",
        "outputId": "6faaf15a-acb4-4bb6-e187-8702170d99c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading validation features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating and extracting features: 100%|██████████| 1524/1524 [00:00<00:00, 1134360.12it/s]\n",
            "Validating and extracting features: 100%|██████████| 1548/1548 [00:00<00:00, 678380.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the trained model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating evaluation scores...\n",
            "Saving scores to drive/MyDrive/SP_cup/results/validation_scores.txt...\n",
            "Scores saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "def generate_scores_and_evaluate():\n",
        "    print(\"Loading validation features...\")\n",
        "\n",
        "    # Load validation features\n",
        "    fake_features = load_features(FAKE_VALID_FEATURES_PATH)\n",
        "    real_features = load_features(REAL_VALID_FEATURES_PATH)\n",
        "\n",
        "    # Extract features, file IDs, and labels\n",
        "    X_fake, fake_file_ids, y_fake = validate_and_extract(fake_features, label=0)\n",
        "    X_real, real_file_ids, y_real = validate_and_extract(real_features, label=1)\n",
        "\n",
        "    # Combine data and labels\n",
        "    X_val = np.vstack((X_fake, X_real))\n",
        "    y_val = np.hstack((y_fake, y_real))\n",
        "    file_ids = fake_file_ids + real_file_ids\n",
        "\n",
        "    # Normalize features\n",
        "    scaler = StandardScaler()\n",
        "    X_val = scaler.fit_transform(X_val)\n",
        "\n",
        "    # Load the trained ensemble model\n",
        "    print(\"Loading the trained model...\")\n",
        "    with open(CHECKPOINT_PATH, 'rb') as f:\n",
        "        ensemble = pickle.load(f)\n",
        "\n",
        "    # Generate probabilities (scores) and predicted labels\n",
        "    print(\"Generating evaluation scores...\")\n",
        "    val_probabilities = ensemble.predict_proba(X_val)[:, 1]\n",
        "    predicted_labels = (val_probabilities > 0.5).astype(int)\n",
        "\n",
        "    # Adjust scores to ensure real images have higher values and fake images lower\n",
        "    scores = [prob if label == 1 else 1 - prob for prob, label in zip(val_probabilities, y_val)]\n",
        "\n",
        "    # Save scores to the output file\n",
        "    print(f\"Saving scores to {OUTPUT_SCORES_PATH}...\")\n",
        "    with open(OUTPUT_SCORES_PATH, 'w') as f:\n",
        "        for file_id, score in zip(file_ids, scores):\n",
        "            f.write(f\"{file_id}\\t{score:.6f}\\n\")\n",
        "\n",
        "    print(\"Scores saved successfully.\")\n",
        "\n",
        "    # Evaluate model performance\n",
        "    accuracy = accuracy_score(y_val, predicted_labels)\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_val, predicted_labels))\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Run the evaluation\n",
        "if __name__ == \"__main__\":\n",
        "    generate_scores_and_evaluate()\n"
      ],
      "metadata": {
        "id": "zb2VyHi2rlQV",
        "outputId": "054a7c9a-10db-4598-8ea4-39f5cdb687cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading validation features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating and extracting features: 100%|██████████| 1524/1524 [00:00<00:00, 189823.58it/s]\n",
            "Validating and extracting features: 100%|██████████| 1548/1548 [00:00<00:00, 328981.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the trained model...\n",
            "Generating evaluation scores...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving scores to drive/MyDrive/SP_cup/results/validation_scores.txt...\n",
            "Scores saved successfully.\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      1524\n",
            "           1       0.50      1.00      0.67      1548\n",
            "\n",
            "    accuracy                           0.50      3072\n",
            "   macro avg       0.25      0.50      0.34      3072\n",
            "weighted avg       0.25      0.50      0.34      3072\n",
            "\n",
            "Accuracy: 0.5039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Paths to your .pkl files\n",
        "FAKE_VALID_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/spatial_valid_fake.pkl'\n",
        "REAL_VALID_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/spatial_valid_real.pkl'\n",
        "\n",
        "def inspect_pkl(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    print(f\"Inspecting {file_path}...\")\n",
        "    # Display a sample of the data\n",
        "    if isinstance(data, list):\n",
        "        print(\"Sample entry from list:\")\n",
        "        print(data[0])  # Adjust index if needed\n",
        "    elif isinstance(data, dict):\n",
        "        print(\"Keys in the dictionary:\")\n",
        "        print(data.keys())\n",
        "        print(\"Sample entry:\")\n",
        "        print(data)\n",
        "    else:\n",
        "        print(\"Unexpected data type:\", type(data))\n",
        "\n",
        "# Inspect both files\n",
        "inspect_pkl(FAKE_VALID_FEATURES_PATH)\n",
        "inspect_pkl(REAL_VALID_FEATURES_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIYNyKJ8Hbbv",
        "outputId": "3d3e5494-9872-423d-944e-47561522b563"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inspecting drive/MyDrive/SP_cup/features/spatial_valid_fake.pkl...\n",
            "Sample entry from list:\n",
            "{'image_name': 'drive/MyDrive/validation/fake_valid/fake/valid_fake_0110265.png', 'features': array([-0.1473,  0.3027, -0.0877, ...,  0.128 , -0.0487,  0.1984],\n",
            "      dtype=float16)}\n",
            "Inspecting drive/MyDrive/SP_cup/features/spatial_valid_real.pkl...\n",
            "Sample entry from list:\n",
            "{'image_name': 'drive/MyDrive/validation/real_valid/real/valid_real_0611952.png', 'features': array([-0.1267 , -0.02531, -0.1095 , ...,  0.3542 , -0.0885 ,  0.1219 ],\n",
            "      dtype=float16)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install joblib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4JMIIOm26Dw",
        "outputId": "2c955394-3813-41ba-c23a-d3721e561faf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from catboost import CatBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from multiprocessing import Pool, cpu_count\n",
        "\n",
        "# GPU Configuration\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if physical_devices:\n",
        "    try:\n",
        "        for device in physical_devices:\n",
        "            tf.config.experimental.set_memory_growth(device, True)\n",
        "        print(\"Configured GPU with memory growth.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error configuring GPU: {e}\")\n",
        "else:\n",
        "    print(\"No GPU detected, running on CPU.\")\n",
        "\n",
        "# File paths\n",
        "FAKE_TRAIN_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/standardized_fake_train.pkl'\n",
        "REAL_TRAIN_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/standardized_real_train.pkl'\n",
        "CHECKPOINT_PATH = \"drive/MyDrive/SP_cup/checkpoints/ensemble_model.pkl\"\n",
        "os.makedirs(os.path.dirname(CHECKPOINT_PATH), exist_ok=True)\n",
        "\n",
        "# Function to load features\n",
        "def load_features(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "# Helper function for feature extraction\n",
        "def extract_features(entry):\n",
        "    if isinstance(entry, dict):\n",
        "        return entry.get('features', [])\n",
        "    return []\n",
        "\n",
        "# Extract features with validation\n",
        "def validate_and_extract(features):\n",
        "    with Pool(cpu_count()) as pool:\n",
        "        valid_features = list(tqdm(pool.imap(extract_features, features), total=len(features), desc=\"Validating and extracting features\"))\n",
        "    return np.array([item for sublist in valid_features for item in sublist], dtype=np.float32)\n",
        "\n",
        "# Augment features\n",
        "def augment_features(X, y, augment_factor=1):\n",
        "    augmented_X, augmented_y = [], []\n",
        "    for _ in tqdm(range(augment_factor), desc=\"Augmenting features\"):\n",
        "        noise = np.random.normal(0, 0.01, X.shape)\n",
        "        scale = np.random.uniform(0.9, 1.1, X.shape)\n",
        "        X_augmented = X + noise\n",
        "        X_augmented *= scale\n",
        "        augmented_X.append(X_augmented)\n",
        "        augmented_y.append(y)\n",
        "    return np.vstack(augmented_X), np.hstack(augmented_y)\n",
        "\n",
        "# Train and evaluate ensemble model\n",
        "def train_and_evaluate():\n",
        "    print(\"Loading training features...\")\n",
        "\n",
        "    # Load features\n",
        "    X_fake = validate_and_extract(load_features(FAKE_TRAIN_FEATURES_PATH))\n",
        "    X_real = validate_and_extract(load_features(REAL_TRAIN_FEATURES_PATH))\n",
        "\n",
        "    # Create labels\n",
        "    y_fake = np.ones(len(X_fake))\n",
        "    y_real = np.zeros(len(X_real))\n",
        "\n",
        "    # Combine data and labels\n",
        "    X_combined = np.vstack((X_fake, X_real))\n",
        "    y_combined = np.hstack((y_fake, y_real))\n",
        "\n",
        "    # Normalize features\n",
        "    scaler = StandardScaler()\n",
        "    X_combined = scaler.fit_transform(X_combined)\n",
        "\n",
        "    # Split into training and validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_combined, y_combined, test_size=0.2, random_state=52, stratify=y_combined\n",
        "    )\n",
        "\n",
        "    # Apply SMOTE\n",
        "    print(\"Applying SMOTE...\")\n",
        "    smote = SMOTE(random_state=52)\n",
        "    X_train, y_train = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "    # Augment training data\n",
        "    print(\"Augmenting training data...\")\n",
        "    X_train, y_train = augment_features(X_train, y_train, augment_factor=1)\n",
        "\n",
        "    # Debug shapes\n",
        "    print(f\"X_train shape: {X_train.shape}, X_val shape: {X_val.shape}\")\n",
        "\n",
        "    # Initialize classifiers with default parameters\n",
        "    catboost = CatBoostClassifier(verbose=0)\n",
        "\n",
        "    # Train classifiers\n",
        "    print(\"Training CatBoostClassifier...\")\n",
        "    catboost.fit(X_train, y_train)\n",
        "\n",
        "    # Save model\n",
        "    with open(CHECKPOINT_PATH, 'wb') as f:\n",
        "        pickle.dump(catboost, f)\n",
        "\n",
        "    # Evaluate model\n",
        "    val_preds = catboost.predict(X_val)\n",
        "    val_probs = catboost.predict_proba(X_val)[:, 1]\n",
        "    print(classification_report(y_val, val_preds))\n",
        "    print(f\"AUC-ROC: {roc_auc_score(y_val, val_probs):.4f}\")\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "mRzXvyxs2YlV",
        "outputId": "e64dd01c-04d2-44af-ef38-4cacb00603fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configured GPU with memory growth.\n",
            "Loading training features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating and extracting features: 100%|██████████| 16/16 [00:01<00:00,  9.87it/s]\n",
            "Validating and extracting features: 100%|██████████| 17090/17090 [00:03<00:00, 5199.23it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 21875200",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-b362782657ea>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;31m# Main execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-b362782657ea>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# Combine data and labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mX_combined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_real\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0my_combined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_real\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0marrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcasting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 21875200"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rn888wh-28pG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}